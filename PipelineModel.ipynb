{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d77d4650"
      },
      "source": [
        "# Notebook Terstruktur untuk STT dan Terjemahan\n",
        "\n",
        "Notebook ini berisi kode untuk pipeline Speech-to-Text (STT) dan Terjemahan (Translation), serta evaluasi performanya menggunakan metrik WER dan BLEU.\n",
        "\n",
        "## 1. Instalasi dan Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a0e3195",
        "outputId": "802a87bd-8b8b-4d60-8137-824118143213"
      },
      "source": [
        "! pip install jiwer sacrebleu librosa tensorflow pandas numpy\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import sacrebleu\n",
        "from jiwer import wer\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.2.1)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.14.1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: rapidfuzz, portalocker, colorama, sacrebleu, jiwer\n",
            "Successfully installed colorama-0.4.6 jiwer-4.0.0 portalocker-3.2.0 rapidfuzz-3.13.0 sacrebleu-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de1d9971"
      },
      "source": [
        "## 2. Memuat Model dan Tokenizer\n",
        "\n",
        "Memuat model STT dan Terjemahan, serta tokenizer yang diperlukan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52894ffd",
        "outputId": "8dd35622-5fef-4c6b-f4c9-7abfc7008794"
      },
      "source": [
        "# Path ke model dan tokenizer Anda di Google Drive\n",
        "stt_model_path = '/content/drive/MyDrive/databaru/stt.h5'\n",
        "terjemahan_model_path = '/content/drive/MyDrive/databaru/terjemahmodel.h5'\n",
        "stt_tokenizer_path = '/content/drive/MyDrive/databaru/tokenizer.pkl'\n",
        "translation_source_tokenizer_path = '/content/drive/MyDrive/databaru/translation_source_tokenizer.pkl'\n",
        "translation_target_tokenizer_path = '/content/drive/MyDrive/databaru/translation_target_tokenizer.pkl'\n",
        "\n",
        "# Muat Model STT dan Terjemahan\n",
        "try:\n",
        "    stt_model = load_model(stt_model_path)\n",
        "    terjemahan_model = load_model(terjemahan_model_path)\n",
        "    print(\"Model STT dan Terjemahan berhasil dimuat.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error memuat model: {e}\")\n",
        "\n",
        "# Muat Tokenizer untuk STT dan Terjemahan\n",
        "try:\n",
        "    with open(stt_tokenizer_path, 'rb') as f:\n",
        "        stt_tokenizer = pickle.load(f)\n",
        "\n",
        "    with open(translation_source_tokenizer_path, 'rb') as f:\n",
        "        translation_source_tokenizer = pickle.load(f)\n",
        "\n",
        "    with open(translation_target_tokenizer_path, 'rb') as f:\n",
        "        translation_target_tokenizer = pickle.load(f)\n",
        "    print(\"Tokenizer berhasil dimuat.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error memuat tokenizer: {e}\")\n",
        "\n",
        "# Define sequence lengths for translation model (these values might need adjustment based on your training data)\n",
        "src_length = 7  # Example source sequence length\n",
        "tar_length = 10  # Example target sequence length\n",
        "print(f\"Panjang sekuens sumber (src_length): {src_length}\")\n",
        "print(f\"Panjang sekuens target (tar_length): {tar_length}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model STT dan Terjemahan berhasil dimuat.\n",
            "Tokenizer berhasil dimuat.\n",
            "Panjang sekuens sumber (src_length): 7\n",
            "Panjang sekuens target (tar_length): 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7920f193"
      },
      "source": [
        "## 3. Definisi Fungsi\n",
        "\n",
        "Mendefinisikan fungsi-fungsi untuk ekstraksi fitur audio, prediksi STT, prediksi terjemahan, dan pipeline keseluruhan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2762689e"
      },
      "source": [
        "# Fungsi untuk mengekstrak fitur audio (gabungan MFCC dan Mel-spectrogram)\n",
        "def extract_audio_features(file_path, max_length=7, n_mels=128, n_mfcc=13):\n",
        "    audio, sr = librosa.load(file_path, sr=None)  # Load dengan sample rate asli\n",
        "\n",
        "    # Ekstrak Mel-spectrogram\n",
        "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=2048, n_mels=n_mels, fmax=sr/2)\n",
        "    mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "    mel_spec = mel_spec.T  # Transpose agar memiliki dimensi (time_steps, n_mels)\n",
        "\n",
        "    # Ekstrak MFCC\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
        "    mfccs = mfccs.T # Transpose agar memiliki dimensi (time_steps, n_mfcc)\n",
        "\n",
        "\n",
        "    # Gabungkan fitur\n",
        "    combined_features = np.concatenate((mel_spec, mfccs), axis=1)\n",
        "\n",
        "    # Padding atau pemotongan jika panjang timesteps tidak sesuai\n",
        "    if combined_features.shape[0] < max_length:\n",
        "        pad_width = max_length - combined_features.shape[0]\n",
        "        combined_features = np.pad(combined_features, ((0, pad_width), (0, 0)), mode='constant')\n",
        "    elif combined_features.shape[0] > max_length:\n",
        "        combined_features = combined_features[:max_length, :]\n",
        "\n",
        "    return combined_features\n",
        "\n",
        "# Fungsi untuk mengonversi audio ke teks (STT)\n",
        "def predict_audio_to_text(stt_model, audio_file):\n",
        "    feature = extract_audio_features(audio_file)\n",
        "    feature = np.expand_dims(feature, axis=0)  # Menambahkan dimensi batch\n",
        "    pred = stt_model.predict(feature)\n",
        "    pred = np.argmax(pred, axis=-1)\n",
        "\n",
        "    # Decode hasil prediksi menjadi teks\n",
        "    pred_text_list = []\n",
        "    for seq in pred:\n",
        "        decoded_seq = [stt_tokenizer.index_word.get(idx, '') for idx in seq if idx != 0]\n",
        "        pred_text_list.append(' '.join(decoded_seq))\n",
        "\n",
        "    return pred_text_list[0]\n",
        "\n",
        "# Fungsi untuk tokenisasi dan encoding input teks\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "    X = tokenizer.texts_to_sequences(lines)\n",
        "    X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=length, padding='post')\n",
        "    return X\n",
        "\n",
        "# Fungsi untuk prediksi terjemahan dengan model\n",
        "def predict_seq(model, tokenizer, source):\n",
        "    prediction = model.predict(source, verbose=0)[0]\n",
        "    integers = [np.argmax(vector) for vector in prediction]\n",
        "    target = [tokenizer.index_word.get(i, '') for i in integers if i != 0]\n",
        "    return ' '.join(target)\n",
        "\n",
        "# Fungsi untuk prediksi terjemahan (dari Bugis ke Indonesia)\n",
        "def translate_text_with_model(translation_model, tokenizer, input_text):\n",
        "    # Ensure input_text is treated as a list for encode_sequences\n",
        "    input_seq = encode_sequences(translation_source_tokenizer, src_length, [input_text])\n",
        "    # The predict_seq function expects a single sequence (batch size 1)\n",
        "    translated_text = predict_seq(translation_model, translation_target_tokenizer, input_seq)\n",
        "    return translated_text\n",
        "\n",
        "\n",
        "# Pipeline untuk audio ke teks ke terjemahan\n",
        "def audio_to_text_translation_pipeline(audio_file_path, stt_model, terjemahan_model, stt_tokenizer,\n",
        "                                       translation_source_tokenizer, translation_target_tokenizer,\n",
        "                                       src_length, tar_length):\n",
        "\n",
        "    # Langkah 1: Prediksi teks Bugis dari file audio menggunakan model STT\n",
        "    predicted_transcript = predict_audio_to_text(stt_model, audio_file_path)\n",
        "    print(\"Transkripsi Bugis (STT Model):\", predicted_transcript)\n",
        "\n",
        "    # Langkah 2: Menerjemahkan teks Bugis ke dalam Bahasa Indonesia menggunakan model Terjemahan\n",
        "    translated_text = translate_text_with_model(terjemahan_model,\n",
        "                                                translation_target_tokenizer,\n",
        "                                                predicted_transcript)\n",
        "    print(\"Terjemahan Bahasa Indonesia (Translation Model):\", translated_text)\n",
        "\n",
        "    return predicted_transcript, translated_text\n",
        "\n",
        "    \"\"\"\n",
        "    Pipeline to process audio file, transcribe to text, and translate the text.\n",
        "\n",
        "    Args:\n",
        "        audio_file_path (str): Path to the input audio file.\n",
        "        stt_model: Loaded STT model.\n",
        "        terjemahan_model: Loaded translation model.\n",
        "        stt_tokenizer: Tokenizer for STT.\n",
        "        translation_source_tokenizer: Tokenizer for the source language (Bugis).\n",
        "        translation_target_tokenizer: Tokenizer for the target language (Indonesia).\n",
        "        src_length (int): Source sequence length for translation model.\n",
        "        tar_length (int): Target sequence length for translation model.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the predicted Bugis transcript and the translated Indonesian text.\n",
        "    \"\"\"\n",
        "\n",
        "# Fungsi untuk menghitung Word Error Rate (WER)\n",
        "def calculate_wer(ground_truth, hypothesis):\n",
        "    \"\"\"\n",
        "    Calculates the Word Error Rate (WER) between a ground truth and a hypothesis sentence.\n",
        "\n",
        "    Args:\n",
        "        ground_truth (str): The correct transcription.\n",
        "        hypothesis (str): The predicted transcription.\n",
        "\n",
        "    Returns:\n",
        "        float: The WER score.\n",
        "    \"\"\"\n",
        "    return wer(ground_truth, hypothesis)\n",
        "\n",
        "# Fungsi untuk menghitung BLEU score\n",
        "def calculate_bleu(ground_truth, hypothesis):\n",
        "    \"\"\"\n",
        "    Calculates the BLEU score between a ground truth and a hypothesis sentence.\n",
        "\n",
        "    Args:\n",
        "        ground_truth (str): The correct translation (reference).\n",
        "        hypothesis (str): The predicted translation.\n",
        "\n",
        "    Returns:\n",
        "        float: The BLEU score.\n",
        "    \"\"\"\n",
        "    # sacrebleu expects ground_truth as a list of references\n",
        "    return sacrebleu.sentence_bleu(hypothesis, [ground_truth]).score\n",
        "\n",
        "# Fungsi untuk memuat data ground truth dari file CSV\n",
        "def load_ground_truth_from_csv(csv_path):\n",
        "    \"\"\"\n",
        "    Loads ground truth data from a CSV file.\n",
        "\n",
        "    Args:\n",
        "        csv_path (str): The path to the CSV file. The CSV should have columns\n",
        "                        'filename', 'stt', and 'translation'.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are filenames and values are dictionaries\n",
        "              containing 'stt' and 'translation' ground truths.\n",
        "    \"\"\"\n",
        "    ground_truth_data = {}\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        for index, row in df.iterrows():\n",
        "            filename = row['filename']\n",
        "            stt_gt = row['stt']\n",
        "            translation_gt = row['translation']\n",
        "            ground_truth_data[filename] = {'stt': stt_gt, 'translation': translation_gt}\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: CSV file not found at {csv_path}\")\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: Missing required column in CSV: {e}. Make sure the CSV has 'filename', 'stt', and 'translation' columns.\")\n",
        "    return ground_truth_data\n",
        "\n",
        "# Fungsi untuk memproses file audio yang dipilih dan mengumpulkan hasil\n",
        "def process_specific_audio_files_for_evaluation(list_of_audio_paths, ground_truth_data, stt_model, terjemahan_model, stt_tokenizer, translation_source_tokenizer, translation_target_tokenizer, src_length, tar_length):\n",
        "    \"\"\"\n",
        "    Processes a list of specific audio files using the audio_to_text_translation_pipeline\n",
        "    and collects predicted and ground truth texts for files with available ground truth.\n",
        "\n",
        "    Args:\n",
        "        list_of_audio_paths (list): A list of full paths to the audio files to process.\n",
        "        ground_truth_data (dict): A dictionary where keys are filenames and values are dictionaries\n",
        "                                  containing 'stt' and 'translation' ground truths.\n",
        "        stt_model: Loaded STT model.\n",
        "        terjemahan_model: Loaded translation model.\n",
        "        stt_tokenizer: Tokenizer for STT.\n",
        "        translation_source_tokenizer: Tokenizer for the source language (Bugis).\n",
        "        translation_target_tokenizer: Tokenizer for the target language (Indonesia).\n",
        "        src_length (int): Source sequence length for translation model.\n",
        "        tar_length (int): Target sequence length for translation model.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing four lists (only for files with ground truth):\n",
        "               - stt_ground_truths (list): List of STT ground truth texts.\n",
        "               - stt_hypotheses (list): List of predicted STT texts.\n",
        "               - translation_ground_truths (list): List of translation ground truth texts.\n",
        "               - translation_hypotheses (list): List of predicted translation texts.\n",
        "    \"\"\"\n",
        "    stt_ground_truths = []\n",
        "    stt_hypotheses = []\n",
        "    translation_ground_truths = []\n",
        "    translation_hypotheses = []\n",
        "\n",
        "    print(\"\\n--- Processing Selected Audio Files ---\")\n",
        "    for file_path in list_of_audio_paths:\n",
        "        filename = os.path.basename(file_path) # Extract filename from path\n",
        "        print(f\"\\nProcessing file: {filename}\")\n",
        "\n",
        "        # Check if ground truth exists for this file before processing\n",
        "        if filename in ground_truth_data:\n",
        "            try:\n",
        "                # Use the pipeline function\n",
        "                predicted_transcript, translated_text = audio_to_text_translation_pipeline(\n",
        "                    file_path,\n",
        "                    stt_model,\n",
        "                    terjemahan_model,\n",
        "                    stt_tokenizer,\n",
        "                    translation_source_tokenizer,\n",
        "                    translation_target_tokenizer,\n",
        "                    src_length,\n",
        "                    tar_length\n",
        "                )\n",
        "                print(f\"Processed successfully: {filename}\")\n",
        "\n",
        "                # Collect ground truth and hypothesis\n",
        "                stt_ground_truths.append(ground_truth_data[filename]['stt'])\n",
        "                stt_hypotheses.append(predicted_transcript)\n",
        "                translation_ground_truths.append(ground_truth_data[filename]['translation'])\n",
        "                translation_hypotheses.append(translated_text)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "        else:\n",
        "            print(f\"Warning: No ground truth data found for {filename}. Skipping processing and evaluation for this file.\")\n",
        "\n",
        "\n",
        "    return stt_ground_truths, stt_hypotheses, translation_ground_truths, translation_hypotheses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe380433"
      },
      "source": [
        "## 4. Contoh Penggunaan Pipeline\n",
        "\n",
        "Contoh penggunaan pipeline untuk memproses satu file audio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eaeb35d",
        "outputId": "00b2d7a9-03fe-4dd8-c24d-c07c3e21ce11"
      },
      "source": [
        "# Contoh penggunaan pipeline\n",
        "# Gantilah dengan path file audio yang sesuai di Google Drive Anda\n",
        "audio_file_path_example = '/content/drive/MyDrive/databaru/dataAudio16Hz/50_copy_1.wav'\n",
        "\n",
        "print(f\"Processing audio file: {audio_file_path_example}\")\n",
        "\n",
        "# Pastikan model dan tokenizer sudah dimuat\n",
        "if 'stt_model' in globals() and 'terjemahan_model' in globals() and 'stt_tokenizer' in globals(\n",
        "    ) and 'translation_source_tokenizer' in globals() and 'translation_target_tokenizer' in globals():\n",
        "    predicted_transcript_example, translated_text_example = audio_to_text_translation_pipeline(\n",
        "        audio_file_path_example,\n",
        "        stt_model,\n",
        "        terjemahan_model,\n",
        "        stt_tokenizer,\n",
        "        translation_source_tokenizer,\n",
        "        translation_target_tokenizer,\n",
        "        src_length,\n",
        "        tar_length\n",
        "    )\n",
        "\n",
        "    print(f\"\\n--- Hasil Pipeline ---\")\n",
        "    print(f\"Transkripsi Bugis: {predicted_transcript_example}\")\n",
        "    print(f\"Terjemahan Bahasa Indonesia: {translated_text_example}\")\n",
        "else:\n",
        "    print(\"Model atau tokenizer belum dimuat. Harap jalankan kembali bagian 2.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing audio file: /content/drive/MyDrive/databaru/dataAudio16Hz/50_copy_1.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "Transkripsi Bugis (STT Model): mitauka jokka dare'e cilaleku\n",
            "Terjemahan Bahasa Indonesia (Translation Model): saya pergi pergi kebun diri mahal\n",
            "\n",
            "--- Hasil Pipeline ---\n",
            "Transkripsi Bugis: mitauka jokka dare'e cilaleku\n",
            "Terjemahan Bahasa Indonesia: saya pergi pergi kebun diri mahal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5b79658"
      },
      "source": [
        "## 5. Evaluasi Performansi\n",
        "\n",
        "Memuat data ground truth, memproses file audio terpilih, dan menghitung metrik WER dan BLEU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2e8a23f",
        "outputId": "51ad0291-fe32-496f-94ac-05c6daa81cb1"
      },
      "source": [
        "# Step 5: Memuat ground truth dari multiple CSV\n",
        "# Define the paths to your two ground truth CSV files\n",
        "ground_truth_csv_paths = [\n",
        "    '/content/drive/MyDrive/skripsi/Book2groundtht.csv',\n",
        "    '/content/drive/MyDrive/skripsi/transkripbarutanpawav.csv',\n",
        "    '/content/drive/MyDrive/databaru/groundtruthpengujian.csv',\n",
        "    '/content/drive/MyDrive/databaru/groundtruthpengujian.csv'\n",
        "]\n",
        "\n",
        "# Load the ground truth data from the list of CSV files\n",
        "combined_ground_truth_data = {}\n",
        "for csv_path in ground_truth_csv_paths:\n",
        "    ground_truth_data_single_csv = load_ground_truth_from_csv(csv_path)\n",
        "    combined_ground_truth_data.update(ground_truth_data_single_csv)\n",
        "\n",
        "print(f\"Loaded and combined ground truth data from {len(ground_truth_csv_paths)} CSV files.\")\n",
        "print(f\"Total entries in combined ground truth data: {len(combined_ground_truth_data)}\")\n",
        "\n",
        "\n",
        "# Step 6: Membuat daftar path audio yang akan dievaluasi\n",
        "# Define a list of specific audio file paths to evaluate\n",
        "selected_audio_files = [\n",
        "    '/content/drive/MyDrive/databaru/dataUji/datauji11.wav',\n",
        "    '/content/drive/MyDrive/databaru/dataUji/datauji12.wav',\n",
        "    '/content/drive/MyDrive/databaru/dataUji/datauji13.wav',\n",
        "    '/content/drive/MyDrive/databaru/dataUji/datauji14.wav',\n",
        "    '/content/drive/MyDrive/databaru/dataUji/datauji15.wav',\n",
        "    '/content/drive/MyDrive/databaru/dataUji/datauji21.wav',\n",
        "    '/content/drive/MyDrive/databaru/dataUji/datauji22.wav',\n",
        "    '/content/drive/MyDrive/databaru/dataUji/datauji23.wav',\n",
        "    '/content/drive/MyDrive/databaru/dataUji/datauji24.wav',\n",
        "    '/content/drive/MyDrive/databaru/dataUji/datauji25.wav',\n",
        "    '/content/drive/MyDrive/databaru/dataUji/datauji31.wav',\n",
        "    '/content/drive/MyDrive/databaru/dataUji/datauji32.wav',\n",
        "    '/content/drive/MyDrive/databaru/dataUji/datauji33.wav',\n",
        "    '/content/drive/MyDrive/databaru/dataUji/datauji34.wav',\n",
        "    '/content/drive/MyDrive/databaru/dataUji/datauji35.wav',\n",
        "    '/content/drive/MyDrive/databaru/dataUji/uji1.wav',\n",
        "    '/content/drive/MyDrive/databaru/dataUji/uji2.wav',\n",
        "    '/content/drive/MyDrive/databaru/dataUji/uji3.wav',\n",
        "    '/content/drive/MyDrive/databaru/dataUji/uji4.wav',\n",
        "    '/content/drive/MyDrive/databaru/dataUji/uji5.wav'\n",
        "]\n",
        "\n",
        "print(f\"\\nDefined a list with {len(selected_audio_files)} audio file paths to evaluate.\")\n",
        "\n",
        "# Process the selected audio files and collect results\n",
        "# Ensure model and tokenizer are loaded\n",
        "if 'stt_model' in globals() and 'terjemahan_model' in globals() and 'stt_tokenizer' in globals() and 'translation_source_tokenizer' in globals() and 'translation_target_tokenizer' in globals():\n",
        "    stt_gts, stt_hyps, trans_gts, trans_hyps = process_specific_audio_files_for_evaluation(\n",
        "        selected_audio_files,\n",
        "        combined_ground_truth_data,\n",
        "        stt_model,\n",
        "        terjemahan_model,\n",
        "        stt_tokenizer,\n",
        "        translation_source_tokenizer,\n",
        "        translation_target_tokenizer,\n",
        "        src_length,\n",
        "        tar_length\n",
        "    )\n",
        "\n",
        "    print(f\"\\nCollected {len(stt_gts)} STT entries with ground truth.\")\n",
        "    print(f\"Collected {len(trans_gts)} Translation entries with ground truth.\")\n",
        "\n",
        "\n",
        "    # Step 7: Menghitung dan menampilkan WER dan BLEU\n",
        "    print(\"\\n--- Evaluation Results ---\")\n",
        "\n",
        "    # Hitung dan tampilkan WER rata-rata\n",
        "    if stt_gts:\n",
        "        average_wer = sum([calculate_wer(gt, hp) for gt, hp in zip(stt_gts, stt_hyps)]) / len(stt_gts)\n",
        "        print(f\"Average WER for STT: {average_wer:.4f}\")\n",
        "    else:\n",
        "        print(\"No STT data available for WER calculation (no selected files had ground truth).\")\n",
        "\n",
        "    # Hitung dan tampilkan BLEU rata-rata\n",
        "    if trans_gts:\n",
        "        # sacrebleu.corpus_bleu expects references as a list of lists\n",
        "        average_bleu = sacrebleu.corpus_bleu(trans_hyps, [trans_gts]).score\n",
        "        print(f\"Corpus BLEU for Translation: {average_bleu:.2f}\")\n",
        "    else:\n",
        "        print(\"No Translation data available for BLEU calculation (no selected files had ground truth).\")\n",
        "else:\n",
        "    print(\"Model atau tokenizer belum dimuat. Harap jalankan kembali bagian 2.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded and combined ground truth data from 4 CSV files.\n",
            "Total entries in combined ground truth data: 3520\n",
            "\n",
            "Defined a list with 20 audio file paths to evaluate.\n",
            "\n",
            "--- Processing Selected Audio Files ---\n",
            "\n",
            "Processing file: datauji11.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "Transkripsi Bugis (STT Model): tennang de'to nasaba ni ni kareba\n",
            "Terjemahan Bahasa Indonesia (Translation Model): perasaanku karena karena karena\n",
            "Processed successfully: datauji11.wav\n",
            "\n",
            "Processing file: datauji12.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "Transkripsi Bugis (STT Model): de' no pasa'e melli\n",
            "Terjemahan Bahasa Indonesia (Translation Model): saya saya beli\n",
            "Processed successfully: datauji12.wav\n",
            "\n",
            "Processing file: datauji13.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "Transkripsi Bugis (STT Model): pake mitai mebbu beppa\n",
            "Terjemahan Bahasa Indonesia (Translation Model): saya ingin membuat membuat\n",
            "Processed successfully: datauji13.wav\n",
            "\n",
            "Processing file: datauji14.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "Transkripsi Bugis (STT Model): alekka ni ni ni ni kareba na\n",
            "Terjemahan Bahasa Indonesia (Translation Model): sandalku pergi pergi saudaranya\n",
            "Processed successfully: datauji14.wav\n",
            "\n",
            "Processing file: datauji15.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "Transkripsi Bugis (STT Model): pake bawang ni ni ni\n",
            "Terjemahan Bahasa Indonesia (Translation Model): kita tidak jalan jalan\n",
            "Processed successfully: datauji15.wav\n",
            "\n",
            "Processing file: datauji21.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "Transkripsi Bugis (STT Model): polo ko lemari baru\n",
            "Terjemahan Bahasa Indonesia (Translation Model): berikan aku baru\n",
            "Processed successfully: datauji21.wav\n",
            "\n",
            "Processing file: datauji22.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "Transkripsi Bugis (STT Model): deppa ka matindro\n",
            "Terjemahan Bahasa Indonesia (Translation Model): saya tidur duluan\n",
            "Processed successfully: datauji22.wav\n",
            "\n",
            "Processing file: datauji23.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "Transkripsi Bugis (STT Model): aja' ka nasaba\n",
            "Terjemahan Bahasa Indonesia (Translation Model): saya buatkan panas\n",
            "Processed successfully: datauji23.wav\n",
            "\n",
            "Processing file: datauji24.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "Transkripsi Bugis (STT Model): aleng i bissai\n",
            "Terjemahan Bahasa Indonesia (Translation Model): berikan buatkan panas\n",
            "Processed successfully: datauji24.wav\n",
            "\n",
            "Processing file: datauji25.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "Transkripsi Bugis (STT Model): aleng i nasaba\n",
            "Terjemahan Bahasa Indonesia (Translation Model): berikan buatkan panas\n",
            "Processed successfully: datauji25.wav\n",
            "\n",
            "Processing file: datauji31.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "Transkripsi Bugis (STT Model): maeloka i i\n",
            "Terjemahan Bahasa Indonesia (Translation Model): besok hari apa\n",
            "Processed successfully: datauji31.wav\n",
            "\n",
            "Processing file: datauji32.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "Transkripsi Bugis (STT Model): yolo ka matindro\n",
            "Terjemahan Bahasa Indonesia (Translation Model): saya tidur duluan\n",
            "Processed successfully: datauji32.wav\n",
            "\n",
            "Processing file: datauji33.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "Transkripsi Bugis (STT Model): aja' ka kaju\n",
            "Terjemahan Bahasa Indonesia (Translation Model): saya tidur duluan\n",
            "Processed successfully: datauji33.wav\n",
            "\n",
            "Processing file: datauji34.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "Transkripsi Bugis (STT Model): upoji ka matindro\n",
            "Terjemahan Bahasa Indonesia (Translation Model): saya tidur duluan\n",
            "Processed successfully: datauji34.wav\n",
            "\n",
            "Processing file: datauji35.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "Transkripsi Bugis (STT Model): engka ka matindro\n",
            "Terjemahan Bahasa Indonesia (Translation Model): saya tidur\n",
            "Processed successfully: datauji35.wav\n",
            "\n",
            "Processing file: uji1.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "Transkripsi Bugis (STT Model): maeloka mitai mebbu beppa\n",
            "Terjemahan Bahasa Indonesia (Translation Model): saya ingin melihatnya membuat kue\n",
            "Processed successfully: uji1.wav\n",
            "\n",
            "Processing file: uji2.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "Transkripsi Bugis (STT Model): maeloka mitai mebbu beppa\n",
            "Terjemahan Bahasa Indonesia (Translation Model): saya ingin melihatnya membuat kue\n",
            "Processed successfully: uji2.wav\n",
            "\n",
            "Processing file: uji3.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "Transkripsi Bugis (STT Model): reppa'i ero ero ku we\n",
            "Terjemahan Bahasa Indonesia (Translation Model): berikan sarung itu itu\n",
            "Processed successfully: uji3.wav\n",
            "\n",
            "Processing file: uji4.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "Transkripsi Bugis (STT Model): marroki mitai ni\n",
            "Terjemahan Bahasa Indonesia (Translation Model): berikan sudah bertelur\n",
            "Processed successfully: uji4.wav\n",
            "\n",
            "Processing file: uji5.wav\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "Transkripsi Bugis (STT Model): tennapodo de'to nakenna ki abala\n",
            "Terjemahan Bahasa Indonesia (Translation Model): semoga kita tidak celaka\n",
            "Processed successfully: uji5.wav\n",
            "\n",
            "Collected 20 STT entries with ground truth.\n",
            "Collected 20 Translation entries with ground truth.\n",
            "\n",
            "--- Evaluation Results ---\n",
            "Average WER for STT: 0.9642\n",
            "Corpus BLEU for Translation: 3.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f222441"
      },
      "source": [
        "## 6. Finish task"
      ]
    }
  ]
}